{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in a Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "Given data on loans that we've given in the past, we want to be able to predict whether a new customer will be given a loan or not. This is to aid decision making when it comes to offering loans at our bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas - for data reading, manipulating, writing, analysis, and some plotting!\n",
    "import pandas as pd \n",
    "\n",
    "# Numpy - for mathematical and matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn - for data visualisation\n",
    "import seaborn as sns\n",
    "\n",
    "# For visualising decision tree\n",
    "import pydotplus\n",
    "import graphviz \n",
    "\n",
    "# Import Decision tree from sklearn \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# For splitting our data into training and testing datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For evaluating our models\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "# To produe graphs here in our notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start by loading our .csv file into our Jupyter notebook\n",
    "loan = pd.read_csv('../loan_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dimensions of the data a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the dataframe\n",
    "loan.head()\n",
    "\n",
    "# Anyone guess what the command for getting the last 5 rows is?\n",
    "# loan_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions of training dataframe\n",
    "loan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high-level information on the columns\n",
    "loan.info() \n",
    "\n",
    "# Explain all this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we want to get some descriptive statistics of the numerical features? Try doing this on your own (Google)\n",
    "loan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "1) Are there any missing/NA values?\n",
    "\n",
    "--> if so, how do we deal with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .isnull() function to return values in the dataframe which are Null/NA. Get first 5 rows of dataframe \n",
    "# only\n",
    "loan.isnull().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the the sum of NA values in each of the columns/features\n",
    "loan.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of where our NA values our located, how do we deal with them?\n",
    "\n",
    "\n",
    "**Note** We engage the learners before each of the following blocks to get them thinking of best approaches\n",
    "\n",
    "**Note2** These are our assumptions. In data cleaning you make assumptions based on common sense, and domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) Dealing with Gender, Married, Loan_Amount_Term ==> We remove those rows because they don't make sense!\n",
    "loan.dropna(subset= ['Gender', 'Married', 'Loan_Amount_Term'], how = 'any', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR DEPENDENTS -- it makes sense to fill NA dependents with 0\n",
    "loan['Dependents'] = loan['Dependents'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR Self-Employed -- it makes sense to fill NA Self-employed values with 0\n",
    "loan['Self_Employed'] = loan['Self_Employed'].fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR LOAN_AMount -- it makes sense to fill NA loans with 0\n",
    "loan['LoanAmount'] = loan['LoanAmount'].fillna(float(0))   # or 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Credit History - same\n",
    "loan['Credit_History'] = loan['Credit_History'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sum of NA values in each column/feature\n",
    "loan.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dimensions now that we've done a bit of cleaning\n",
    "loan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas plot function \n",
    "loan.Loan_Status.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using seaborn instead - a library built for data visualisation\n",
    "l_status = sns.countplot(x = 'Loan_Status', data = loan)\n",
    "\n",
    "l_status.set_title('Distribution of Loan status in our data')  # First, type variable name, and press tab --> \n",
    "                                                                #    Jupyter notebook is so useful!\n",
    "l_status.set_ylabel('Frequency', fontsize = 18)\n",
    "l_status.set_xlabel('Loan Status', fontsize = 18)\n",
    "l_status.tick_params(labelsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Countplot of Loan status while accounting for Gender\n",
    "ls_gender = sns.countplot(x = 'Loan_Status', hue = 'Gender', data = loan)\n",
    "\n",
    "ls_gender.set_title('Loan Status and Gender') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.2 - maybe we want to graph horizontally instead?\n",
    " \n",
    "sns.set_style('darkgrid') # or try 'whitegrid' (Basic point is seaborn is flexible style-wise)\n",
    "\n",
    "ls_gender_horizontal = sns.countplot(y = 'Loan_Status', hue = 'Gender', data = loan, palette= None)\n",
    "\n",
    "ls_gender_horizontal.set_title('Loan Status and Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Property area and No. of dependents for both class values (i.e. Loan status = Yes, and Loan_Status = No)\n",
    "\n",
    "area_gender = sns.catplot(x = 'Dependents', hue = 'Property_Area', col = 'Loan_Status', data = loan, kind = 'count', \n",
    "            palette= 'rainbow')\n",
    "\n",
    "area_gender\n",
    "# See if you can add a title, and change the font sizes of the X and Y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore other relationships using same syntax. \n",
    "\n",
    "With every graph, try adding a title, and changing the X and Y labels appropriately. Feel free to play around with seaborn palettes and styles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comment \n",
    "# Let them tell you what to type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to include the thing we want to predict as the input data, so lets drop it. Also let's put the classes into their own variable for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python has many data types. Let's explore the distribution of different data types across our features.\n",
    "loan.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's drop the ID column as it adds no useful information \n",
    "loan = loan.drop(['Loan_ID'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now to split our data in several ways: \n",
    "\n",
    "1) We need to split by **features** (all columns but *Loan Status*) and our **target variable**, *Loan Status*\n",
    "\n",
    "\n",
    "2) We need to split by rows into one dataset that we will train our model on -- **the training set** -- and one which our model will not see and on which we test performance -- **the testing set**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we split by features and target variable\n",
    "\n",
    "loan_feats = loan.drop(['Loan_Status'], axis= 1)\n",
    "\n",
    "loan_class = loan['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we split into training and testing sets!\n",
    "train_feats, test_feats, train_class, test_class = train_test_split(loan_feats, loan_class, test_size = 0.3, random_state = 123)\n",
    "\n",
    "# EXPLAIN parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "Mention that we'll be using decision trees. The model we'll be using can't handle object (or non-numeric) data types!\n",
    "\n",
    "==> Therefore, we'll need to convert all to numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the data into a form our Decision tree can use, we need to use **one hot encoding**. This basically means each value a feature can take, now becomes a column in its own right. \n",
    "\n",
    "**Note** Important to show shape before and after so it can sink in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding all features in training set\n",
    "train_feats = pd.get_dummies(train_feats)\n",
    "\n",
    "# One-hot encoding all features in testing set\n",
    "test_feats = pd.get_dummies(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the data looks after the transformation\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the same with the test set. Note that the same features were used, and hence the same output is expected.\n",
    "test_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how columns look in training set\n",
    "train_feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how columns look in testing set\n",
    "test_feats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if they're all numeric now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gloss over these different numeric data types\n",
    "train_feats.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target attribute values from Y/N to 1/0 -- training set\n",
    "train_class = np.where(train_class == 'Y', 1,0)\n",
    "train_class.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target attribute values from Y/N to 1/0 -- testing set\n",
    "test_class = np.where(test_class == 'Y', 1,0)\n",
    "test_class.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model! (Note use of sklearn)\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Now we fit/train it on our data. Mention X = train_feats, Y = train_class. \n",
    "tree_model.fit(train_feats, train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define a function for plotting our decision tree!\n",
    "def plotTree():\n",
    "    dot_data = export_graphviz(tree_model, \n",
    "                                    out_file=None, \n",
    "                                    feature_names=train_feats.columns,\n",
    "                                    filled=True, \n",
    "                                    rounded=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    return graph \n",
    "\n",
    "# NOTE This is quite hard to follow. So mention that this will be explored in more detail in modules to come \n",
    "# (Classification specifically), but this session is meant illustrate ART OF THE POSSIBLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE ON PREPARATION \n",
    "\n",
    "**Need to do this prior to session and then remove this block of code from notebook**\n",
    "\n",
    "Do the following: \n",
    "\n",
    "1) You need to download the .zip graphviz file (again) from https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "\n",
    "\n",
    "2) In the notebook, just before you run the _plotTree()_ function, you do:\n",
    "        - import os\n",
    "        - os.environ[\"PATH\"] += os.pathstep + r\"C:\\Users\\Client\\Downloads\\graphviz-2.38\\release\\bin\"\n",
    "        - plotTree()\n",
    "\n",
    "**Note** the path in the second part is just where you extract the zip file. On the Windows laptop I couldn't extract it to the working directory of the notebook because of admin restrictions, but I could extract it in the Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge tree! Very complex. Not very useful since it's not very understandable (defeating the original motive for\n",
    "using a decision tree in the first place!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another tree. This time, let's set a limit to the **max_depth** parameter, to avoid creating a very complex tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model 2. Setting max_depth at 3 (arbitrary choice)\n",
    "tree_model2 = DecisionTreeClassifier(max_depth= 3)\n",
    "\n",
    "# Fitting the model to the same data as before\n",
    "tree_model2.fit(train_feats, train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTree2():\n",
    "    dot_data = export_graphviz(tree_model2, \n",
    "                                    out_file=None, \n",
    "                                    feature_names=train_feats.columns,\n",
    "                                    filled=True, \n",
    "                                    rounded=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    return graph \n",
    "\n",
    "plotTree2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Much better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our models, it's time to put them to the test. We'll do this by predicting test set values and comparing those predictions to the values we already know are the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions made using model 1 (complex one)\n",
    "predictions1 = tree_model.predict(test_feats)\n",
    "predictions1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare those values with the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate how this model did\n",
    "# Note - refer back to library call\n",
    "\n",
    "print(confusion_matrix(test_class, predictions1))  \n",
    "print(classification_report(test_class, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction using model 2 (simpler)\n",
    "predictions2 = tree_model2.predict(test_feats)\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate this model\n",
    "print(confusion_matrix(test_class, predictions2))  \n",
    "print(classification_report(test_class, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which model is better? \n",
    "> A: Model 2 (less complex)\n",
    "\n",
    "\n",
    "Q: Why though? Isn't the more complex model supposed to be better?\n",
    "> A: Nope. Explain concept of variance vs. bias. Mention that decision trees are actually high variance models that overfit to our training data, and don't perform well on data they haven't seen (like the test set in our case)\n",
    "\n",
    "Link to *Stupid Data Miner tricks paper* (Overfitting the S&P 500)\n",
    "https://www.researchgate.net/publication/247907373_Stupid_Data_Miner_Tricks_Overfitting_the_SP_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
